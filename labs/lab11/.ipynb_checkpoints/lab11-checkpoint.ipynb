{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO408 Lab Exercise 11: Apache Spark\n",
    "\n",
    "The aim of this lab exercise is to explore the features of Apache Spark for processing large data sets.\n",
    "\n",
    "This Jupyter notebook is designed to interact with Apache Spark. It uses Python 3.6.\n",
    "\n",
    "## A quick reminder of how to use Jupyter notebooks\n",
    "\n",
    "Type `Control-Enter` in a cell to run the code. If you see `[*]` to the left of a cell being run, the code is still running. (The Spark commands in Exercise 2 analyse half a million email messages, and will therefore take some time to run.)\n",
    "\n",
    "Use the **Insert** menu to add new cells.\n",
    "\n",
    "Save your progress using **File** ▷ **Save and Checkpoint**.\n",
    "\n",
    "When you have finished this lab, you can create a (non-interactive) copy of this notebook in HTML format using **File** ▷ **Download as…**\n",
    "\n",
    "To exit, select **File** ▷ **Close and Halt**. Do not worry if error messages are displayed in the terminal window from which you started the cluster. You can then type `Control-C` followed by `docker-compose down` to shut down the cluster.\n",
    "\n",
    "## Useful links\n",
    "\n",
    "Note that this exercise uses version 2.4.3 of Spark, not version 3.\n",
    "\n",
    " * [Spark RDD programming guide](https://spark.apache.org/docs/2.4.3/rdd-programming-guide.html)\n",
    " * [Python API documentation for the RDD class](https://spark.apache.org/docs/2.4.3/api/python/pyspark.html#pyspark.RDD)\n",
    " * [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/2.4.3/sql-programming-guide.html)\n",
    " * [Python API documentation for the DataFrame class](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    " * [Python standard library (standard datatypes and functions)](https://docs.python.org/3.6/library/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Exploring the Shakespeare data set\n",
    "\n",
    "In this exercise you will use a data file (`shakespeare.txt`) containing the complete works of William Shakespeare (plays and poems). This comprises about 8.8 MB of semi-structured text.\n",
    "\n",
    "To query the data using Spark, run the code in each of the following cells by clicking on a cell and then typing `Control-Enter` (`⌘-Enter` on a Mac). Then look at the output, and try to understand what the code is doing.\n",
    "\n",
    "The variable `sc` is predefined by PySpark to hold a reference to a *Spark Context* object. This is configured to run Spark in the cluster that you created by running `docker-compose up`. Each node in the cluster is available to store parts of the data and to run computations on them. You can also submit jobs to the cluster via `pyspark` (the Python Spark interpreter) running inside this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"/mnt/data/shakespeare.txt\")\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `lines` now holds a reference to an object of class RDD: a *resilient distributed dataset*. The dataset is automatically partitioned with each partition held in memory in one of the executor processes.\n",
    "\n",
    "Now let us see how many lines of data were read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below calls the `take(n)` method. This retrieves the first *n* data items from the RDD and returns them as a Python list, so the list must be small enough to fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `\\t` represents a tab character.\n",
    "\n",
    "Note that each line begins with a *key*: the name of the work (a play or poem) followed by `@` and a byte offset from the start of the work.\n",
    "\n",
    "Now let us see how the data has been partitioned by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of `getNumPartitions()` shows that the data has been broken up into separate partitions by Spark. The number of partitions would probably be larger if you were running this code on a “real” Spark cluster.\n",
    "\n",
    "Next, to see the range of methods (functions associated with a class) that are available to be called on an RDD object, position the cursor after the `.` in the cell below and press the `Tab` key (you may need to delete and re-enter the `.` for this to work, and it may be slow to respond). Use the `Page Down` key to see all the methods, and `Esc` to close the list. If you want to read what these methods do, see the [documentation for the DataFrame class](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrame).\n",
    "\n",
    "Note: You do not need to *run* the cell below, but if you do, you will get an error message unless you complete the line to call an RDD class method with valid arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us get rid of the key at the start of each line in the data set. Compare the result with that from `lines.take(10)` above. The `map` method takes a function as an argument and applies it to each item in the data set (in this case, a line of text). A new RDD is returned with each item replaced by its mapped value.\n",
    "\n",
    "Here we do the mapping using a lambda function (see Lab 4). This takes one string argument (`line`), splits it at the first (`maxsplit=1`) tab character (`sep=\"\\t\"`) and returns the second substring (`[1]`, because Python indexes lists from 0). As you can see from the result this is not perfect, as there are still some lines that start with a tab character, but it is good enough for our purposes. (If you are interested, investigate Python’s `re` module that provides advanced pattern matching and substitution using regular expressions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "lines2 = lines.map(lambda line: line.split(sep=\"\\t\", maxsplit=1)[1])\n",
    "lines2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python’s `lambda` syntax is very useful for passing short unnamed functions to functions like map and reduce. The general syntax is:\n",
    "\n",
    "```python\n",
    "  lambda arg1, ..., argn: SomeExpressionUsingTheArguments\n",
    "```\n",
    "\n",
    "The expression is evaluated and its result is returned when the function is called.\n",
    "\n",
    "Now we will create a new RDD containing the description of the plays’ scenes. The `collect` method retrieves all data from an RDD and returns it as a Python list. You should therefore `collect` if the RDD is small enough to fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = lines2.filter(lambda line: \"SCENE \" in line)\n",
    "scenes.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate a sorted RDD of pairs (count, word) for each word in the data set. `sortByKey(ascending=False)` means that the result will be sorted in *descending* order, i.e., most frequently used words first. If you want to see the *least* frequently used words, change `False` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = \".?,:;-\"\n",
    "remove_punct_mapping = dict.fromkeys(map(ord, punctuation)) # Do not worry about what this means\n",
    "wordcounts = \\\n",
    "    lines2.map(lambda x: x.translate(remove_punct_mapping).lower()) \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .map(lambda x: (x[1],x[0])) \\\n",
    "    .sortByKey(ascending=False)\n",
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backslash characters at the end of some of the lines above tell Python that the current line of code continues on the next line. This is useful when we have to chain together a long series of method calls.\n",
    "\n",
    "An alternative approach to these long call chains would be to assign the result of each method call to a new variable, and progressively call the methods on each intermediate RDD.\n",
    "\n",
    "By the way, Spark uses a “lazy” approach to generating RDDs, similar to PETL (Lab 4). It usually does no computation on data (including the generation of intermediate RDDs) until some results are actually requested, e.g. by calling `take` or `collect`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how the code above works, you can try a simpler example from http://www.mccarroll.net/blog/pyspark2/ (“Revisiting the wordcount example”). We have reproduced the code from that web page below, starting from the second shaded box onwards (note that the code for removing punctuation is different from the improved version above). Read the description of each step and then try running it.\n",
    "\n",
    "**Code from the second shaded box:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.parallelize(['Its fun to have fun,','but you have to know how.']) \n",
    "wordcounts = lines.map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower()) \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x,y:x+y) \\\n",
    "    .map(lambda x:(x[1],x[0])) \\\n",
    "    .sortByKey(False) \n",
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for step 1: `map(<function>)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = lines.map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\n",
    "r1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for step 2: `flatMap(<function>)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r1.flatMap(lambda x: x.split())\n",
    "r2.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for step 3: `map(<function>)` (again)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = r2.map(lambda x: (x, 1))\n",
    "r3.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for step 4: `reduceByKey(<function>)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r4 = r3.reduceByKey(lambda x,y:x+y)\n",
    "r4.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for step 5: `map(<function>)` (yet again!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r5 = r4.map(lambda x:(x[1],x[0]))\n",
    "r5.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for step 6: `sortByKey( ascending=True|False )`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r6 = r5.sortByKey(ascending=False)\n",
    "r6.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, leaving that example, make sure that you understand the difference between the functions `map` and `flatMap`, by predicting what each of the following three cells will do, and then running them to check whether your prediction is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.parallelize([\"aardvark\", \"baby\", \"carrot\"]) # Create an RDD from a Python list\n",
    "word_chars = words.map(list)  # The function 'list' converts a string to a list of characters (which are just short strings)\n",
    "word_chars.collect() # Collect gathers the RDD into a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = words.flatMap(list)\n",
    "all_chars.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_chars = all_chars.distinct()\n",
    "distinct_chars.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Exploring the Enron dataset\n",
    "\n",
    "Next, you will explore a dataset of email messages from the Enron corporation. You can read about it [here](https://en.wikipedia.org/wiki/Enron_Corpus).\n",
    "\n",
    "First, remove the Exercise 1 dataset from memory. The output should be 0, indicating the success of the garbage collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -sf\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than reading the data into a Spark resilient distributed dataset (RDD), we will  create a Spark *data frame* from the JSON data file. A data frame includes schema information (inferred from the JSON file in this case).\n",
    "\n",
    "The `printSchema()` method call below prints the schema, and the `first()` method call prints the first message in the data set. The variable `spark` is predefined by PySpark to hold a reference to a *Spark Session* object, which is used instead of the Spark context to create a data frame.\n",
    "\n",
    "This code will probably while take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = spark.read.json('/mnt/data/messages.json')\n",
    "messages.createOrReplaceTempView(\"messages\") # This is needed before doing any subsequent SQL queries on this data frame\n",
    "print(messages, '\\n')\n",
    "messages.printSchema() # Print the schema\n",
    "print('Number of messages:', messages.count())\n",
    "messages.first() # Look at the first email message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark data frames support the use of SQL (extended to allow nested column/property names, e.g. `headers.From`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the total number of message senders in the data set\n",
    "spark.sql(\"select count(distinct headers.From) fromcount from messages\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is expressed as a DataFrame that comprises a single row.\n",
    "\n",
    "There are also a range of useful methods we can use on our data frame, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count emails from louis.soldano@...\n",
    "messages.filter(messages.headers.From == 'louis.soldano@enron.com').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "\n",
    "Answer the questions below by writing your Python code in the corresponding cells below. Save the modified notebook and upload it to Blackboard. You do not need to provide the output from the code. The deadline to submit your answers is **Thursday 1 October at 5pm.**\n",
    "\n",
    "Explore the use of SQL queries and/or dataframe and RDD methods to summarise and extract information from the Enron dataset. The following web page might be useful to see what operations can be performed on data frames and RDDs: https://github.com/lgallen/pyspark_dataframes/blob/master/housing_model.ipynb (section “Dataframe operations”).\n",
    "\n",
    "Documentation for the methods that can be called on data frames: https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "\n",
    "Documentation for the methods that can be called on RDDs: http://spark.apache.org/docs/2.4.3/api/python/pyspark.html#pyspark.RDD\n",
    "\n",
    "Some built-in Spark functions: http://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#module-pyspark.sql.functions\n",
    "\n",
    "Once you have tried some basic queries, try to answer the questions below. It is OK to report on partially successful attempts! You can add commentary in code comments or additional text cells. Also feel free to invent your own questions about this dataset to answer using Spark.\n",
    "\n",
    "<ol>\n",
    "    <li>\n",
    "        <p><strong>What is the distribution of message body word counts across all messages?</strong> In other words, create an RDD or data frame that maps word counts (how many words are in a message body) to the number of messages with that word count. Rather than giving exact word counts, round them to the closest power of 10 (you can use <code>round(count, -1)</code>). Note that <code>messages</code> is a data frame, whereas this problem might be easily solved using the methods that can be called on an RDD. However, you can extract an RDD from the data frame, e.g:</p>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messagesRdd = messages.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li style=\"list-style: none;\">\n",
    "        <p>The new RDD will contain <code>Row</code> objects. If you call <code>map()</code> on <code>messagesRdd</code> as defined above, the function you provide to <code>map</code> will need to take a row object as its parameter, e.g. <code>messagesRdd.map(lambda row: ...)</code>. See the <a href=\"https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.Row\" target=\"_blank\">Row class documentation</a> to find out how to access the columns of a row (basically, it is the same as for a data frame).</p>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to answer question 1.\n",
    "split_messages = messagesRdd.map(lambda row: row.body.split())\n",
    "\n",
    "shakespeare_count=split_messages.map(lambda  word:(word))\n",
    "\n",
    "shakespeare_count_RBK=shakespeare_count.map(lambda x: round(len(x),-1))\n",
    "\n",
    "final = shakespeare_count_RBK.map(lambda x: (x, 1)) \\\n",
    "        .reduceByKey(lambda x,y:x+y)\n",
    "\n",
    "final2 = final.sortByKey(True)\n",
    "final.take(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol start=\"2\">\n",
    "    <li>\n",
    "        <p><strong>Which words appear most commonly within message subject lines?</strong></p>\n",
    "        <p><strong>Hint:</strong> this is very similar to the standard map-reduce example of counting words in documents — see the word count example in Exercise 1. The data processed in that example is an RDD, whereas we have a data frame, so you may want to use <code>messagesRdd</code> as defined above.</p>\n",
    "        <p><strong>Tip for sorting an RDD:</strong> To sort an RDD containing key-value pairs (represented in Python as <code>(key, value)</code> tuples), you can use the RDD method <code>sortBy()</code>. To sort by <em>value</em> in descending order, call <code>sortBy(lambda pair: pair[1], ascending=False)</code>.</p>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to answer question 2.\n",
    "punctuation = \".?,:;-\"\n",
    "remove_punct_mapping = dict.fromkeys(map(ord, punctuation)) # Do not worry about what this means\n",
    "wordcounts = \\\n",
    "    messagesRdd.map(lambda x: x.headers.Subject) \\\n",
    "    .map(lambda x: x.translate(remove_punct_mapping).lower()) \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .sortBy(lambda pair: pair[1], ascending=False)\n",
    "    \n",
    "wordcounts.first()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol start=\"3\">\n",
    "    <li><strong>Which senders sent the most messages?</strong></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to answer question 3.\n",
    "senderCounts = \\\n",
    "    messagesRdd.map(lambda x: x.headers.From) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .max(lambda x: x[1]) \\\n",
    "    \n",
    "senderCounts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge question (not assessed)\n",
    "\n",
    "<p><strong>What are the unique sender/recipient pairs together with the number of times each pair occurs?</strong></p>\n",
    "<p><strong>Note:</strong> This query requires parsing the <code>headers.To</code> string to find the individual recipients, as in general an email message may have more than one recipient, separated by commas followed by a space, tab or newline character. Extracting individual recipients can be done using map-reduce on the RDD underlying the data frame. We can also do this using the <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html#lateral-view\" target=\"_blank\">Spark SQL <em>lateral view</em> construct</a> and the <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode\" target=\"_blank\"><code>explode</code></a> and <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.split\" target=\"_blank\"><code>split</code></a> functions.</p>\n",
    "<p>The following query creates a new data frame with columns named <code>sender</code> and <code>recipient</code>. There is a row for each occurrence of a sender-recipient pair in the data set (there may be multiple entries for any given pair). (Note that the column names <code>from</code> and <code>to</code> have been changed to <code>sender</code> and <code>recipient</code> to avoid confusing Spark SQL, which does not like a column named <code>from</code>.)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_recipient_pairs = spark.sql(\"select messages.headers.From as sender, recipient, count(*) as count from messages lateral view explode(split(messages.headers.To, '[,\\\\\\\\s]+')) tolist as recipient group by sender, recipient order by count desc\")\n",
    "sender_recipient_pairs.createOrReplaceTempView(\"sender_recipient_pairs\")\n",
    "sender_recipient_pairs.printSchema()\n",
    "sender_recipient_pairs.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The next cell produces a new data frame that excludes the emails sent or CCed to self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_self_mails_sender_recipient_pairs = spark.sql(\"select sender, recipient, count from sender_recipient_pairs where sender <> recipient\")\n",
    "no_self_mails_sender_recipient_pairs.createOrReplaceTempView(\"no_self_mails_sender_recipient_pairs\")\n",
    "no_self_mails_sender_recipient_pairs.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just shown you how to answer this question using data frames, so try to answer this question using a different technique (e.g., convert the dataframe to an RDD and use RDD methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to answer question 4.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
