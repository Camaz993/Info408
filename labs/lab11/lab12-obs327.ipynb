{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 408 Lab Exercise 12: Apache Spark continued\n",
    "\n",
    "This lab exercise continues the exploration of Apache Spark started in Lab 11. For convenience, the instructions for Lab 10 are included [at the end](#INFO408-Lab-Exercise-11:-Apache-Spark) so that you can refer to the information given there, and continue working on that lab exercise if you did not finish it last week.\n",
    "\n",
    "## IMPORTANT\n",
    "\n",
    "**This notebook is for use only with the computers in lab OBS 327.** It is configured only for that location and will not work correctly elsewhere. If you want to use Docker (with or without VirtualBox) on your own computer, download `lab12-docker.ipynb` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipients in common\n",
    "\n",
    "The following Spark query will recreate the three data frames that you used to explore the Enron dataset in Lab 11, except this time the first line only keeps a sample of 10% of the messages (the `False` in the first argument of the `sample` method means that sampling with replacement is **not** used). The final data frame, `no_self_mails_sender_recipient_pairs` contains a row for each `sender`/`recipient` pair in the 10% sample of the Enron dataset. A `count` column records how many emails were sent from the sender to the recipient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = spark.read.json(\"/mnt/t-drive/InfoSci/INFO408/Spark/messages.json\").sample(False, 0.1)\n",
    "messages.createOrReplaceTempView(\"messages\") # This is needed before doing an SQL query\n",
    "sender_recipient_pairs = spark.sql(\n",
    "    \"select messages.headers.From as sender, recipient, count(*) as count \"\n",
    "    \"from messages lateral view explode(split(messages.headers.To, '[,\\\\\\\\s]+')) tolist as recipient \"\n",
    "    \"group by sender, recipient order by count desc\"\n",
    ")\n",
    "sender_recipient_pairs.createOrReplaceTempView(\"sender_recipient_pairs\")\n",
    "no_self_mails_sender_recipient_pairs = spark.sql(\n",
    "    \"select sender, recipient, count from sender_recipient_pairs \"\n",
    "     \"where sender != recipient\"\n",
    ")\n",
    "print \"Schema:\"\n",
    "sender_recipient_pairs.printSchema()\n",
    "no_self_mails_sender_recipient_pairs.createOrReplaceTempView(\"no_self_mails_sender_recipient_pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Can you generate an RDD in which each element contains a unique pair of users (senders) and a list of the email addresses (recipients) that they have both sent messages to? In other words, for each pair of senders, which recipients do they have in common?\n",
    "\n",
    "See [this article](http://stevekrenzel.com/finding-friends-with-mapreduce) about a map-reduce algorithm for finding *friends in common* (scroll down past the word count example). This is the same example that was discussed in Lecture 11.\n",
    "\n",
    "Here is a Spark implementation of the friends in common algorithm (with an error corrected):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "friends = sc.parallelize([\n",
    "    ('a',['b','c','d']),\n",
    "    ('b',['a','c','d','e']),\n",
    "    ('c',['a','b','d','e']),\n",
    "    ('d',['a','b','c','e']),\n",
    "    ('e',['b','c','d']),\n",
    "    ('z', ['e'])\n",
    "])\n",
    "mapped = friends.flatMap(lambda pair: [(tuple(sorted([pair[0], friend])), set(pair[1])) for friend in pair[1]])\n",
    "grouped = mapped.groupByKey().mapValues(list)\n",
    "grouped_no_singletons = grouped.mapValues(lambda l: l if len(l) == 2 else l + [set()])\n",
    "intersected = grouped_no_singletons.map(lambda pair: (pair[0], reduce(set.intersection, pair[1])))\n",
    "intersected.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you just need to create a friends RDD from the messages data set, with a similar structure to the input to the `sc.parallelize` call above: each entry will be a pair of the form *(sender, list of all recipients that sender has sent messages to)*. Then you can use the code in the cell above to answer the question.\n",
    "\n",
    "You can start by converting the `no_self_mails_sender_recipient_pairs` data frame to an RDD by using the `rdd` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = no_self_mails_sender_recipient_pairs.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that this RDD will contain `Row` objects (see Lab 11, Exercise 2).\n",
    "\n",
    "You might like to immediately replace each row in the RDD with a tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_rdd = rdd.map(lambda row: (row.sender, row.recipient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can call `tuple_rdd.groupByKey().mapValues(list)` to get the RDD you need as input to the friends in common algorithm. *Technical detail*: `mapValues(list)` is needed because the RDD’s `groupByKey` method creates groupings that are not lists; rather they are Python “iterables”. `mapValues(list)` turns the iterables into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphframes\n",
    "\n",
    "Now we will use a third-party `graphframes` library that will let us view the data as a graph or network with a directed edge between each sender and recipient. First we must create data frames representing the vertices and edges of the graph, and these must have specific column names (`id` for the vertices, and `src` and `dst` for the edges). Then we can search for pairs of users (a, b) that satisfy the pattern `(a)-[]->(b); !(b)-[]->(a)`. This means that there is an edge (i.e., emails were sent) from `a` to `b`, but not from `b` to `a`.\n",
    "\n",
    "This query may take some time, so you might like to read about the [graphframes library](https://graphframes.github.io/graphframes/docs/_site/user-guide.html) while you wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "\n",
    "no_self_mails_sender_recipient_pairs.createOrReplaceTempView(\"no_self_mails_sender_recipient_pairs\")\n",
    "vertices = spark.sql(\n",
    "    \"select sender as id from no_self_mails_sender_recipient_pairs \"\n",
    "    \"union \"\n",
    "    \"select recipient from no_self_mails_sender_recipient_pairs\")\n",
    "edges = no_self_mails_sender_recipient_pairs.toDF('src','dst','count')\n",
    "gf = GraphFrame(vertices, edges)\n",
    "gf.find('(a)-[]->(b); !(b)-[]->(a)').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you think of other interesting queries to try using `find` and graph motifs? \n",
    "\n",
    "Other useful `GraphFrame` attributes and methods (besides `find`) include `inDegrees` (for each user, how many other users have mailed the user) and `outDegrees` (for each user, how many other users have been emailed by that user), `pageRank(tol=...)`, and `stronglyConnectedComponents(maxIter)`  (where `maxIter` is the maximum number of iterations to run). Note that `inDegrees` and `outDegrees` are graphframe *attributes*: no brackets are added after these.\n",
    "\n",
    "`pageRank(tol=...)` is the original web page ranking algorithm used by Google. In the context of the Enron emails, the results could be interpreted as the probability for each user that a piece of gossip randomly arriving at one of the users will be passed on through a chain of emails to that specified user (See [Wikipedia](https://en.wikipedia.org/wiki/PageRank)). The `tol` parameter specifies how much error can be tolerated (e.g., 0.01). The result of `pageRank` is another `GraphFrame`. You can find the page rank for the first vertex using `pageRankResult.vertices.take(1)`.\n",
    "\n",
    "See [Wikipedia](https://en.wikipedia.org/wiki/Strongly_connected_component) for a definition of strongly connected components. Running `stronglyConnectedComponents(maxIter)` on the Enron data (even with our 10% sample) may take a long time, depending on what value you provide for the `maxIter` argument. Also, rather than printing the components, it might be best just to count them, using `.count()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out other graphframe queries here. Insert more cells if you need to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that we can use `graphframes` to solve the “friends in common” problem. Here we use a small test `GraphFrame` so the computation is quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Vertex DataFrame\n",
    "v = sqlContext.createDataFrame([\n",
    "    (\"a\", \"Alice\", 34),\n",
    "    (\"b\", \"Bob\", 36),\n",
    "    (\"c\", \"Charlie\", 30),\n",
    "    (\"d\", \"David\", 29),\n",
    "    (\"e\", \"Esther\", 32),\n",
    "    (\"f\", \"Frances\", 36)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Edge DataFrame\n",
    "e = sqlContext.createDataFrame([\n",
    "    (\"a\", \"b\", \"friend\"),\n",
    "    (\"a\", \"f\", \"friend\"),\n",
    "    (\"b\", \"c\", \"friend\"),\n",
    "    (\"c\", \"b\", \"friend\"),\n",
    "    (\"c\", \"f\", \"friend\"),\n",
    "    (\"f\", \"c\", \"friend\"),\n",
    "    (\"e\", \"f\", \"friend\"),\n",
    "    (\"e\", \"d\", \"friend\"),\n",
    "    (\"d\", \"a\", \"friend\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "gf2 = GraphFrame(v,e)\n",
    "gf2.find('(x)-[]->(z); (y)-[]->(z)').filter(\"x != y\").groupBy(\"x\", \"y\").agg(F.collect_set(\"z\")).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO408 Lab Exercise 11: Apache Spark\n",
    "\n",
    "The aim of this lab exercise is to explore the features of Apache Spark for processing large data sets.\n",
    "\n",
    "This Jupyter notebook is designed to interact with Apache Spark. It uses Python 3.6.\n",
    "\n",
    "## A quick reminder of how to use Jupyter notebooks\n",
    "\n",
    "Type `Control-Enter` in a cell to run the code. If you see `[*]` to the left of a cell being run, the code is still running. (The Spark commands in Exercise 2 analyse half a million email messages, and will therefore take some time to run.)\n",
    "\n",
    "Use the **Insert** menu to add new cells.\n",
    "\n",
    "Save your progress using **File** ▷ **Save and Checkpoint**.\n",
    "\n",
    "When you have finished this lab, you can create a (non-interactive) copy of this notebook in HTML format using **File** ▷ **Download as…**\n",
    "\n",
    "To exit, select **File** ▷ **Close and Halt**. Do not worry if error messages are displayed in the terminal window from which you started the cluster. You can then type `Control-C` followed by `docker-compose down` to shut down the cluster.\n",
    "\n",
    "## Useful links\n",
    "\n",
    "Note that this exercise uses version 2.4.3 of Spark, not version 3.\n",
    "\n",
    " * [Spark RDD programming guide](https://spark.apache.org/docs/2.4.3/rdd-programming-guide.html)\n",
    " * [Python API documentation for the RDD class](https://spark.apache.org/docs/2.4.3/api/python/pyspark.html#pyspark.RDD)\n",
    " * [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/2.4.3/sql-programming-guide.html)\n",
    " * [Python API documentation for the DataFrame class](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    " * [Python standard library (standard datatypes and functions)](https://docs.python.org/3.6/library/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Exploring the Shakespeare data set\n",
    "\n",
    "In this exercise you will use a data file (`shakespeare.txt`) containing the complete works of William Shakespeare (plays and poems). This comprises about 8.8 MB of semi-structured text.\n",
    "\n",
    "To query the data using Spark, run the code in each of the following cells by clicking on a cell and then typing `Control-Enter` (`⌘-Enter` on a Mac). Then look at the output, and try to understand what the code is doing.\n",
    "\n",
    "The variable `sc` is predefined by PySpark to hold a reference to a *Spark Context* object. This is configured to run Spark in the cluster that you created by running `docker-compose up`. Each node in the cluster is available to store parts of the data and to run computations on them. You can also submit jobs to the cluster via `pyspark` (the Python Spark interpreter) running inside this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"/mnt/t-drive/InfoSci/INFO408/Spark/shakespeare.txt\")\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `lines` now holds a reference to an object of class RDD: a *resilient distributed dataset*. The dataset is automatically partitioned with each partition held in memory in one of the executor processes.\n",
    "\n",
    "Now let us see how many lines of data were read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below calls the `take(n)` method. This retrieves the first *n* data items from the RDD and returns them as a Python list, so the list must be small enough to fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `\\t` represents a tab character.\n",
    "\n",
    "Note that each line begins with a *key*: the name of the work (a play or poem) followed by `@` and a byte offset from the start of the work.\n",
    "\n",
    "Now let us see how the data has been partitioned by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of `getNumPartitions()` shows that the data has been broken up into separate partitions by Spark. The number of partitions would probably be larger if you were running this code on a “real” Spark cluster.\n",
    "\n",
    "Next, to see the range of methods (functions associated with a class) that are available to be called on an RDD object, position the cursor after the `.` in the cell below and press the `Tab` key (you may need to delete and re-enter the `.` for this to work, and it may be slow to respond). Use the `Page Down` key to see all the methods, and `Esc` to close the list. If you want to read what these methods do, see the [documentation for the DataFrame class](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrame).\n",
    "\n",
    "Note: You do not need to *run* the cell below, but if you do, you will get an error message unless you complete the line to call an RDD class method with valid arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us get rid of the key at the start of each line in the data set. Compare the result with that from `lines.take(10)` above. The `map` method takes a function as an argument and applies it to each item in the data set (in this case, a line of text). A new RDD is returned with each item replaced by its mapped value.\n",
    "\n",
    "Here we do the mapping using a lambda function (see Lab 4). This takes one string argument (`line`), splits it at the first (`maxsplit=1`) tab character (`sep=\"\\t\"`) and returns the second substring (`[1]`, because Python indexes lists from 0). As you can see from the result this is not perfect, as there are still some lines that start with a tab character, but it is good enough for our purposes. (If you are interested, investigate Python’s `re` module that provides advanced pattern matching and substitution using regular expressions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "lines2 = lines.map(lambda line: line.split(\"\\t\", 1)[1])\n",
    "lines2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python’s `lambda` syntax is very useful for passing short unnamed functions to functions like map and reduce. The general syntax is:\n",
    "\n",
    "```python\n",
    "  lambda arg1, ..., argn: SomeExpressionUsingTheArguments\n",
    "```\n",
    "\n",
    "The expression is evaluated and its result is returned when the function is called.\n",
    "\n",
    "Now we will create a new RDD containing the description of the plays’ scenes. The `collect` method retrieves all data from an RDD and returns it as a Python list. You should therefore `collect` if the RDD is small enough to fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = lines2.filter(lambda line: \"SCENE \" in line)\n",
    "scenes.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate a sorted RDD of pairs (count, word) for each word in the data set. `sortByKey(ascending=False)` means that the result will be sorted in *descending* order, i.e., most frequently used words first. If you want to see the *least* frequently used words, change `False` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = \".?,:;-\"\n",
    "remove_punct_mapping = dict.fromkeys(map(ord, punctuation)) # Do not worry about what this means\n",
    "wordcounts = \\\n",
    "    lines2.map(lambda x: x.translate(remove_punct_mapping).lower()) \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .map(lambda x: (x[1],x[0])) \\\n",
    "    .sortByKey(ascending=False)\n",
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backslash characters at the end of some of the lines above tell Python that the current line of code continues on the next line. This is useful when we have to chain together a long series of method calls.\n",
    "\n",
    "An alternative approach to these long call chains would be to assign the result of each method call to a new variable, and progressively call the methods on each intermediate RDD.\n",
    "\n",
    "By the way, Spark uses a “lazy” approach to generating RDDs, similar to PETL (Lab 4). It usually does no computation on data (including the generation of intermediate RDDs) until some results are actually requested, e.g. by calling `take` or `collect`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how the code above works, you can try a simpler example from http://www.mccarroll.net/blog/pyspark2/ (“Revisiting the wordcount example”). We have reproduced the code from that web page below, starting from the second shaded box onwards (note that the code for removing punctuation is different from the improved version above). Read the description of each step and then try running it.\n",
    "\n",
    "**Code from the second shaded box:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.parallelize(['Its fun to have fun,','but you have to know how.']) \n",
    "wordcounts = lines.map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower()) \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x,y:x+y) \\\n",
    "    .map(lambda x:(x[1],x[0])) \\\n",
    "    .sortByKey(False) \n",
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for step 1: `map(<function>)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = lines.map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\n",
    "r1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for step 2: `flatMap(<function>)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r1.flatMap(lambda x: x.split())\n",
    "r2.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for step 3: `map(<function>)` (again)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = r2.map(lambda x: (x, 1))\n",
    "r3.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for step 4: `reduceByKey(<function>)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r4 = r3.reduceByKey(lambda x,y:x+y)\n",
    "r4.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for step 5: `map(<function>)` (yet again!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r5 = r4.map(lambda x:(x[1],x[0]))\n",
    "r5.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for step 6: `sortByKey( ascending=True|False )`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r6 = r5.sortByKey(ascending=False)\n",
    "r6.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, leaving that example, make sure that you understand the difference between the functions `map` and `flatMap`, by predicting what each of the following three cells will do, and then running them to check whether your prediction is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.parallelize([\"aardvark\", \"baby\", \"carrot\"]) # Create an RDD from a Python list\n",
    "word_chars = words.map(list)  # The function 'list' converts a string to a list of characters (which are just short strings)\n",
    "word_chars.collect() # Collect gathers the RDD into a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = words.flatMap(list)\n",
    "all_chars.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_chars = all_chars.distinct()\n",
    "distinct_chars.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Exploring the Enron dataset\n",
    "\n",
    "Next, you will explore a dataset of email messages from the Enron corporation. You can read about it [here](https://en.wikipedia.org/wiki/Enron_Corpus).\n",
    "\n",
    "First, remove the Exercise 1 dataset from memory. The output should be 0, indicating the success of the garbage collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -sf\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than reading the data into a Spark resilient distributed dataset (RDD), we will  create a Spark *data frame* from the JSON data file. A data frame includes schema information (inferred from the JSON file in this case).\n",
    "\n",
    "The `printSchema()` method call below prints the schema, and the `first()` method call prints the first message in the data set. The variable `spark` is predefined by PySpark to hold a reference to a *Spark Session* object, which is used instead of the Spark context to create a data frame.\n",
    "\n",
    "This code will probably while take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = spark.read.json('/mnt/t-drive/InfoSci/INFO408/Spark/messages.json')\n",
    "messages.createOrReplaceTempView(\"messages\") # This is needed before doing any subsequent SQL queries on this data frame\n",
    "print messages, '\\n'\n",
    "messages.printSchema() # Print the schema\n",
    "print 'Number of messages:', messages.count()\n",
    "messages.first() # Look at the first email message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark data frames support the use of SQL (extended to allow nested column/property names, e.g. `headers.From`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the total number of message senders in the data set\n",
    "spark.sql(\"select count(distinct headers.From) fromcount from messages\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is expressed as a DataFrame that comprises a single row.\n",
    "\n",
    "There are also a range of useful methods we can use on our data frame, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count emails from louis.soldano@...\n",
    "messages.filter(messages.headers.From == 'louis.soldano@enron.com').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "\n",
    "Answer the questions below by writing your Python code in the corresponding cells below. Save the modified notebook and upload it to Blackboard. You do not need to provide the output from the code. The deadline to submit your answers is **Thursday 1 October at 5pm.**\n",
    "\n",
    "Explore the use of SQL queries and/or dataframe and RDD methods to summarise and extract information from the Enron dataset. The following web page might be useful to see what operations can be performed on data frames and RDDs: https://github.com/lgallen/pyspark_dataframes/blob/master/housing_model.ipynb (section “Dataframe operations”).\n",
    "\n",
    "Documentation for the methods that can be called on data frames: https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "\n",
    "Documentation for the methods that can be called on RDDs: http://spark.apache.org/docs/2.4.3/api/python/pyspark.html#pyspark.RDD\n",
    "\n",
    "Some built-in Spark functions: http://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#module-pyspark.sql.functions\n",
    "\n",
    "Once you have tried some basic queries, try to answer the questions below. It is OK to report on partially successful attempts! You can add commentary in code comments or additional text cells. Also feel free to invent your own questions about this dataset to answer using Spark.\n",
    "\n",
    "<ol>\n",
    "    <li>\n",
    "        <p><strong>What is the distribution of message body word counts across all messages?</strong> In other words, create an RDD or data frame that maps word counts (how many words are in a message body) to the number of messages with that word count. Rather than giving exact word counts, round them to the closest power of 10 (you can use <code>round(count, -1)</code>). Note that <code>messages</code> is a data frame, whereas this problem might be easily solved using the methods that can be called on an RDD. However, you can extract an RDD from the data frame, e.g:</p>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messagesRdd = messages.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li style=\"list-style: none;\">\n",
    "        <p>The new RDD will contain <code>Row</code> objects. If you call <code>map()</code> on <code>messagesRdd</code> as defined above, the function you provide to <code>map</code> will need to take a row object as its parameter, e.g. <code>messagesRdd.map(lambda row: ...)</code>. See the <a href=\"https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.Row\" target=\"_blank\">Row class documentation</a> to find out how to access the columns of a row (basically, it is the same as for a data frame).</p>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to answer question 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol start=\"2\">\n",
    "    <li>\n",
    "        <p><strong>Which words appear most commonly within message subject lines?</strong></p>\n",
    "        <p><strong>Hint:</strong> this is very similar to the standard map-reduce example of counting words in documents — see the word count example in Exercise 1. The data processed in that example is an RDD, whereas we have a data frame, so you may want to use <code>messagesRdd</code> as defined above.</p>\n",
    "        <p><strong>Tip for sorting an RDD:</strong> To sort an RDD containing key-value pairs (represented in Python as <code>(key, value)</code> tuples), you can use the RDD method <code>sortBy()</code>. To sort by <em>value</em> in descending order, call <code>sortBy(lambda pair: pair[1], ascending=False)</code>.</p>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to answer question 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol start=\"3\">\n",
    "    <li><strong>Which senders sent the most messages?</strong></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to answer question 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge question (not assessed)\n",
    "\n",
    "<p><strong>What are the unique sender/recipient pairs together with the number of times each pair occurs?</strong></p>\n",
    "<p><strong>Note:</strong> This query requires parsing the <code>headers.To</code> string to find the individual recipients, as in general an email message may have more than one recipient, separated by commas followed by a space, tab or newline character. Extracting individual recipients can be done using map-reduce on the RDD underlying the data frame. We can also do this using the <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html#lateral-view\" target=\"_blank\">Spark SQL <em>lateral view</em> construct</a> and the <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode\" target=\"_blank\"><code>explode</code></a> and <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.split\" target=\"_blank\"><code>split</code></a> functions.</p>\n",
    "<p>The following query creates a new data frame with columns named <code>sender</code> and <code>recipient</code>. There is a row for each occurrence of a sender-recipient pair in the data set (there may be multiple entries for any given pair). (Note that the column names <code>from</code> and <code>to</code> have been changed to <code>sender</code> and <code>recipient</code> to avoid confusing Spark SQL, which does not like a column named <code>from</code>.)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_recipient_pairs = spark.sql(\"select messages.headers.From as sender, recipient, count(*) as count from messages lateral view explode(split(messages.headers.To, '[,\\\\\\\\s]+')) tolist as recipient group by sender, recipient order by count desc\")\n",
    "sender_recipient_pairs.createOrReplaceTempView(\"sender_recipient_pairs\")\n",
    "sender_recipient_pairs.printSchema()\n",
    "sender_recipient_pairs.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The next cell produces a new data frame that excludes the emails sent or CCed to self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_self_mails_sender_recipient_pairs = spark.sql(\"select sender, recipient, count from sender_recipient_pairs where sender <> recipient\")\n",
    "no_self_mails_sender_recipient_pairs.createOrReplaceTempView(\"no_self_mails_sender_recipient_pairs\")\n",
    "no_self_mails_sender_recipient_pairs.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just shown you how to answer this question using data frames, so try to answer this question using a different technique (e.g., convert the dataframe to an RDD and use RDD methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to answer the challenge question.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
