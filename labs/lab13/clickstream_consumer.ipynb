{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 408 Lab Exercise 13: Clickstream Analysis in Python using Apache Spark and Apache Kafka\n",
    "\n",
    "This is a clickstream processing demo using Apache Kafka and Spark Structured Streaming. It is based on the original Scala version described at https://github.com/IBM/kafka-streaming-click-analysis/blob/master/README.md. The clickstream data is from Wikipedia, and is streamed line-by-line into a Kafka topic called `clickstream` by the script `clickstream_producer.py` (see the main lab document). Each line comprises four tab-separated values: the previous page visited (`prev`), the current page (`curr`), the type of page (`type`), and the number of clicks for that navigation path (`n`). (In other words, how many times have people navigated from the `prev` page to the `curr` page?)\n",
    "\n",
    "The following code cell subscribes to the `clickstream` topic, extracts the columns from the received records, and starts an in-memory query (more or less like an SQL view) to summarise the most popular destination pages in terms of clicks. The in-memory query can then be queried as an SQL “table” by Spark Structured Streaming.\n",
    "\n",
    "It will normally take a few seconds to start the query, because we are running the entire cluster one one machine. Normally the Spark and Kafka nodes would be separate servers. **The second code cell will not do anything until the first cell has finished.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as psf\n",
    "\n",
    "# Subscribe to the \"clickstream\" topic.\n",
    "records = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"clickstream\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "messages = (\n",
    "    records\n",
    "    # Extract the individual columns from the current line.\n",
    "    .withColumn(\"prev\", psf.split(records.value, \"\\t\")[0])\n",
    "    .withColumn(\"curr\", psf.split(records.value, \"\\t\")[1])\n",
    "    .withColumn(\"type\", psf.split(records.value, \"\\t\")[2])\n",
    "    .withColumn(\"n\", psf.split(records.value, \"\\t\")[3])\n",
    "    # Group by the current page.\n",
    "    .groupBy(\"curr\")\n",
    "    # Sum the number of clicks.\n",
    "    .agg(psf.sum(\"n\").alias(\"num_clicks\"))\n",
    "    # Sort ascending.\n",
    "    .orderBy(\"num_clicks\", ascending=False)\n",
    ")\n",
    "\n",
    "# Create an in-memory query (view) called \"clicks\".\n",
    "query = (\n",
    "    messages.writeStream\n",
    "    .queryName(\"clicks\")\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell loops for a while (until `terminate`, default 2 minutes), printing out the results of the query every few seconds (default 5). Ideally, this should grab the query results and do something useful like plot them in a bar chart. This is left as an exercise for the reader ☺.\n",
    "\n",
    "You can change the following values in the code to change its behaviour:\n",
    "* `seconds` (line 4) is how long the consumer will run for in seconds\n",
    "* `refresh` (line 5) is the refresh rate in seconds, but it will probably take longer than that in practice because we are running the entire cluster on one machine\n",
    "\n",
    "Try changing the SQL `select` statement on line 8 below.\n",
    "\n",
    "Of course, you are fairly limited in what you can change here. To make larger changes you will need to change how the in-memory query is generated by modifying the definition of the `messages` variable on lines 14–27 of the previous code cell. You will also need to stop the query (run the second-last code cell below) and restart it (run the previous code cell again) before you can see any changes.\n",
    "\n",
    "**Remember that if you want to preserve any changes you make to this notebook, you must use the `docker cp` command to copy it back to your `lab13` directory (see the main lab document).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "terminate = datetime.now() + timedelta(seconds=120)\n",
    "refresh = 5\n",
    "\n",
    "while datetime.now() < terminate:\n",
    "    result = spark.sql(\"select * from clicks\")\n",
    "    result.show()\n",
    "    print(\"==========\")\n",
    "    time.sleep(refresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminate the query when done. **Only do this when you are sure you are finished with the query.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For office use only ☺\n",
    "\n",
    "Query monitoring and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query.isActive)\n",
    "print(query.name)\n",
    "print(query.id)\n",
    "# spark.streams.get(query.id).stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
